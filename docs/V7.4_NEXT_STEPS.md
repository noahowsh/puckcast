# V7.4 Development Plan

> **Created**: December 5, 2025
> **Current Model**: V7.3 at 60.49% accuracy
> **Goal**: Improve accuracy, log-loss, and ROC-AUC

---

## Current Performance (Verified Baseline)

| Metric | V7.0 | V7.3 | Target |
|--------|------|------|--------|
| **Accuracy** | 60.24% | 60.49% | 62%+ |
| **Log Loss** | 0.6682 | 0.6702 | <0.660 |
| **ROC-AUC** | 0.6417 | 0.6402 | >0.65 |
| **Brier Score** | 0.2370 | ~0.237 | <0.230 |

---

## Priority 1: Infrastructure Improvements

### 1.1 Dataset Caching (Speed Up Training)

**Problem**: Feature engineering takes ~48 minutes (fetching + computing 222 features)

**Solution**: Cache the complete built dataset after feature engineering

```python
# Proposed: src/nhl_prediction/dataset_cache.py
CACHE_PATH = Path("data/cache/dataset_v7.3.parquet")

def get_or_build_dataset(seasons, force_rebuild=False):
    """Load cached dataset or build fresh."""
    if CACHE_PATH.exists() and not force_rebuild:
        return load_cached_dataset(CACHE_PATH)

    dataset = build_dataset(seasons)
    save_dataset(dataset, CACHE_PATH)
    return dataset
```

**Benefits**:
- Training runs: 48 min → <1 min
- Faster iteration on model experiments
- Version-controlled dataset snapshots

### 1.2 Model Artifact Storage

**Current**: No saved trained model
**Proposed**: Save trained models with metadata

```
data/models/
├── v7.3_20251205.pkl        # Trained model
├── v7.3_20251205_meta.json  # Metrics, features, hyperparams
└── v7.4_experiment_1.pkl    # Experiments
```

---

## Priority 2: Model Improvement Approaches

### 2.1 Gradient Boosting (Most Promising)

**Why**: Handles non-linear relationships, feature interactions natively

**Options**:
- **XGBoost**: Well-tested, good defaults
- **LightGBM**: Faster, handles categorical features
- **CatBoost**: Best for categorical, no preprocessing

**Expected improvement**: +0.5-1.5pp accuracy

**Implementation**:
```python
from lightgbm import LGBMClassifier

model = LGBMClassifier(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.05,
    num_leaves=31,
    min_child_samples=20,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
```

### 2.2 Ensemble Methods

**Why**: Combine multiple model strengths

**Options**:
1. **Stacking**: LogReg + XGBoost + LightGBM → Meta-learner
2. **Weighted Average**: Simple blend of predictions
3. **Feature-level stacking**: Each model's predictions as features

**Expected improvement**: +0.3-0.8pp accuracy

### 2.3 Feature Engineering v2

**Underexplored areas**:
1. **Shot quality metrics**: Shot location, rebound, rush shots
2. **Special teams depth**: PP1 vs PP2 performance
3. **Goalie-specific situational**: Save% by shot type
4. **Player availability**: Top-6 forwards, Top-4 D healthy
5. **Referee tendencies**: Penalty rates by ref crew

### 2.4 Calibration Improvements

**Current issue**: Log-loss got slightly worse (0.6682 → 0.6702)

**Options**:
1. **Platt Scaling**: Fit sigmoid on validation predictions
2. **Isotonic Regression**: Non-parametric calibration (already using)
3. **Temperature Scaling**: Single parameter calibration
4. **Beta Calibration**: More flexible than Platt

---

## Priority 3: Data Pipeline Improvements

### 3.1 Incremental Updates

**Problem**: Full rebuild needed for each prediction day

**Solution**: Incremental feature updates
```python
def update_features_for_date(date):
    """Only compute features for new games."""
    existing = load_cached_features()
    new_games = fetch_games_since(existing.last_date)
    updated = compute_features(new_games)
    return pd.concat([existing, updated])
```

### 3.2 Real-time Prediction Pipeline

**For production**:
1. Pre-game: Load cached team stats
2. At game time: Fetch confirmed lineups/goalies
3. Generate prediction with latest data

---

## Experiment Tracker

| Experiment | Branch | Status | Accuracy | Notes |
|------------|--------|--------|----------|-------|
| V7.4 LightGBM | TBD | Planned | - | Non-linear model |
| V7.4 XGBoost | TBD | Planned | - | Gradient boosting |
| V7.4 Ensemble | TBD | Planned | - | Model stacking |
| V7.4 Calibration | TBD | Planned | - | Temperature scaling |

---

## Implementation Order

### Week 1: Infrastructure
1. [ ] Implement dataset caching
2. [ ] Add model artifact storage
3. [ ] Create experiment tracking framework

### Week 2: Gradient Boosting
1. [ ] LightGBM baseline
2. [ ] Hyperparameter tuning
3. [ ] Compare to LogReg

### Week 3: Ensemble & Calibration
1. [ ] Stacking with LogReg + LightGBM
2. [ ] Temperature scaling
3. [ ] Evaluate combined approach

---

## Success Criteria

**V7.4 ships if**:
- Accuracy ≥ 61.5% (at least +1pp over V7.3)
- Log-loss ≤ 0.660 (improvement over V7.0)
- A+ bucket ≥ 72% accuracy
- Reproducible results across 3 training runs

---

## Notes

- Previous V7.4-V7.6 experiments all FAILED because they added features to LogReg
- Key insight: **Model architecture change** likely more impactful than more features
- LogReg ceiling appears to be ~60.5% with current feature set
- Non-linear models can capture interactions we tried to engineer manually
