# V7.4 Development Plan

> **Created**: December 5, 2025
> **Current Model**: V7.3 at 60.49% accuracy
> **Goal**: Improve accuracy, log-loss, and ROC-AUC

---

## Current Performance (Verified Baseline)

| Metric | V7.0 | V7.3 | Target |
|--------|------|------|--------|
| **Accuracy** | 60.24% | 60.49% | 62%+ |
| **Log Loss** | 0.6682 | 0.6702 | <0.660 |
| **ROC-AUC** | 0.6417 | 0.6402 | >0.65 |
| **Brier Score** | 0.2370 | ~0.237 | <0.230 |

---

## Priority 1: Infrastructure Improvements

### 1.1 Dataset Caching (Speed Up Training)

**Problem**: Feature engineering takes ~48 minutes (fetching + computing 222 features)

**Solution**: Cache the complete built dataset after feature engineering

```python
# Proposed: src/nhl_prediction/dataset_cache.py
CACHE_PATH = Path("data/cache/dataset_v7.3.parquet")

def get_or_build_dataset(seasons, force_rebuild=False):
    """Load cached dataset or build fresh."""
    if CACHE_PATH.exists() and not force_rebuild:
        return load_cached_dataset(CACHE_PATH)

    dataset = build_dataset(seasons)
    save_dataset(dataset, CACHE_PATH)
    return dataset
```

**Benefits**:
- Training runs: 48 min → <1 min
- Faster iteration on model experiments
- Version-controlled dataset snapshots

### 1.2 Model Artifact Storage

**Current**: No saved trained model
**Proposed**: Save trained models with metadata

```
data/models/
├── v7.3_20251205.pkl        # Trained model
├── v7.3_20251205_meta.json  # Metrics, features, hyperparams
└── v7.4_experiment_1.pkl    # Experiments
```

---

## Priority 2: Model Improvement Approaches

### 2.1 Gradient Boosting (Most Promising)

**Why**: Handles non-linear relationships, feature interactions natively

**Options**:
- **XGBoost**: Well-tested, good defaults
- **LightGBM**: Faster, handles categorical features
- **CatBoost**: Best for categorical, no preprocessing

**Expected improvement**: +0.5-1.5pp accuracy

**Implementation**:
```python
from lightgbm import LGBMClassifier

model = LGBMClassifier(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.05,
    num_leaves=31,
    min_child_samples=20,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
```

### 2.2 Ensemble Methods

**Why**: Combine multiple model strengths

**Options**:
1. **Stacking**: LogReg + XGBoost + LightGBM → Meta-learner
2. **Weighted Average**: Simple blend of predictions
3. **Feature-level stacking**: Each model's predictions as features

**Expected improvement**: +0.3-0.8pp accuracy

### 2.3 Feature Engineering v2

**Underexplored areas**:
1. **Shot quality metrics**: Shot location, rebound, rush shots
2. **Special teams depth**: PP1 vs PP2 performance
3. **Goalie-specific situational**: Save% by shot type
4. **Player availability**: Top-6 forwards, Top-4 D healthy
5. **Referee tendencies**: Penalty rates by ref crew

### 2.4 Calibration Improvements

**Current issue**: Log-loss got slightly worse (0.6682 → 0.6702)

**Options**:
1. **Platt Scaling**: Fit sigmoid on validation predictions
2. **Isotonic Regression**: Non-parametric calibration (already using)
3. **Temperature Scaling**: Single parameter calibration
4. **Beta Calibration**: More flexible than Platt

---

## Priority 3: Data Pipeline Improvements

### 3.1 Incremental Updates

**Problem**: Full rebuild needed for each prediction day

**Solution**: Incremental feature updates
```python
def update_features_for_date(date):
    """Only compute features for new games."""
    existing = load_cached_features()
    new_games = fetch_games_since(existing.last_date)
    updated = compute_features(new_games)
    return pd.concat([existing, updated])
```

### 3.2 Real-time Prediction Pipeline

**For production**:
1. Pre-game: Load cached team stats
2. At game time: Fetch confirmed lineups/goalies
3. Generate prediction with latest data

---

## Experiment Tracker

| Experiment | Branch | Status | Accuracy | Notes |
|------------|--------|--------|----------|-------|
| V7.4 LightGBM | claude/general-session | **FAILED** | 57.15% | -3.34pp vs LogReg |
| V7.4 XGBoost | TBD | Planned | - | Gradient boosting |
| V7.4 Ensemble | TBD | Planned | - | Model stacking |
| V7.4 Calibration | TBD | Planned | - | Temperature scaling |

### V7.4 LightGBM Results (Verified December 5, 2025)

**Result: FAILED - LightGBM performs worse than LogReg**

| Metric | V7.3 LogReg | V7.4 LightGBM | Difference |
|--------|-------------|---------------|------------|
| Accuracy | 60.49% | 57.15% | **-3.34pp** |
| Log Loss | 0.6702 | 0.7023 | +0.0321 (worse) |
| ROC-AUC | 0.6402 | 0.5931 | -0.0471 (worse) |
| Brier Score | ~0.237 | 0.2517 | +0.0147 (worse) |

**Hyperparameters tested:**
- Config 1: n_est=100, depth=4, lr=0.1 -> CV: 57.76%
- Config 2: n_est=200, depth=5, lr=0.05 -> CV: 58.82%
- Config 3: n_est=300, depth=6, lr=0.03 -> CV: 58.94% (best)
- Config 4: n_est=500, depth=4, lr=0.02 -> CV: 58.78%

**Key insight**: LightGBM overfits on training data. The regularized LogReg (C=0.05) handles the noisy NHL prediction task better than default gradient boosting parameters.

---

## Implementation Progress

### Infrastructure (Completed)
1. [x] Implement dataset caching (`src/nhl_prediction/dataset_cache.py`)
2. [ ] Add model artifact storage
3. [ ] Create experiment tracking framework

### Gradient Boosting (In Progress)
1. [x] LightGBM baseline - **FAILED** (57.15% vs 60.49%)
2. [ ] More aggressive regularization for LightGBM
3. [ ] XGBoost with tuned regularization
4. [ ] Compare to LogReg

### Ensemble & Calibration (Planned)
1. [ ] Stacking with LogReg + LightGBM
2. [ ] Temperature scaling
3. [ ] Evaluate combined approach

---

## Success Criteria

**V7.4 ships if**:
- Accuracy ≥ 61.5% (at least +1pp over V7.3)
- Log-loss ≤ 0.660 (improvement over V7.0)
- A+ bucket ≥ 72% accuracy
- Reproducible results across 3 training runs

---

## Notes

- Previous V7.4-V7.6 experiments all FAILED because they added features to LogReg
- Key insight: **Model architecture change** was hypothesized to be more impactful than more features
- LogReg ceiling appears to be ~60.5% with current feature set
- **Dec 5, 2025**: LightGBM experiment FAILED - gradient boosting actually performed 3.34pp WORSE than LogReg
- **New insight**: NHL prediction may be inherently noisy - regularized linear models may be optimal for this domain
- LightGBM likely overfits to noise in training data; LogReg's strong L2 regularization (C=0.05) prevents this
- Next experiments should focus on: (1) more aggressive regularization for tree models, (2) ensemble approaches, (3) calibration improvements
